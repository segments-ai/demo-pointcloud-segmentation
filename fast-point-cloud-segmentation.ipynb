{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFgYIvjQnufM"
      },
      "source": [
        "# Fast Point Cloud Segmentation with Model-Assisted Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-0awlVnufQ"
      },
      "source": [
        "Annotating point clouds is difficult and time-consuming, especially if you have to annotate every single point (for semantic/panoptic segmentation). In order to label a large dataset, you need to have access to a large labeling team, or have a lot of patience.\n",
        "\n",
        "Luckily, you can speed up the labeling process through model-assisted labeling. Instead of annotating every point cloud from scratch, you first train a model on a small number of labeled point clouds and then use model predictions to speed up the rest of the labeling. After correcting some of the model predictions, you can retrain your model to improve the predictions, and so on. This is called **model-assisted labeling**.\n",
        "\n",
        "This notebook shows how you can set up model-assisted labeling on [Segments.ai](https://segments.ai?utm_source=guide&utm_medium=colab&utm_campaign=mal-pc-seg) using its simple Python SDK. As an example, we'll label a set of diverse frames of the SemanticKITTI dataset by using model predictions from [SqueezeSegV3](https://arxiv.org/abs/2004.01803).\n",
        "\n",
        "If you want to label objects using 3D bounding boxes instead, take a look at our other [notebook](https://colab.research.google.com/drive/1OGHJeaVU3geXmQDuW4UdhLprbNyfbflg)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4hoeJwVnufR"
      },
      "source": [
        "## 1. Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diS8HwT_nufS"
      },
      "source": [
        "We'll start by installing the Python SDK and cloning the demo repository from Github.\n",
        "\n",
        "If you're using Colab, be sure to use a GPU-powered runtime, so you can run the segmentation model later. You can change your runtype by clicking on `Runtime > Change runtime type` in the top bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsyzKLmHnufS"
      },
      "outputs": [],
      "source": [
        "! pip install segments-ai -q\n",
        "! git clone https://github.com/segments-ai/demo-pointcloud-segmentation.git -q\n",
        "%cd demo-pointcloud-segmentation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj6m-meinufT"
      },
      "source": [
        "When the SDK is installed, we can initialize the client with an API key. You can find your API keys in your [account settings](https://segments.ai/account). If you don't have an account yet, you can create one on [here](https://segments.ai/join)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTfnUNnynufT"
      },
      "outputs": [],
      "source": [
        "from segments import SegmentsClient\n",
        "\n",
        "api_key = \"API_KEY\"\n",
        "client = SegmentsClient(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf6ypiIxnufU"
      },
      "source": [
        "Next, we'll clone an example dataset containing some SemanticKITTI frames.\n",
        "\n",
        "If you want to create a dataset containing your own point clouds, check out the code snippets [in the appendix](#create-your-own-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDrlf4psnufU"
      },
      "outputs": [],
      "source": [
        "clone = client.clone_dataset(\n",
        "    \"admin-tobias/semantickitti\",\n",
        "    new_name=\"fast-labeling-semantickitti\",\n",
        "    new_public=False,\n",
        ")\n",
        "dataset_identifier = f\"{clone.owner}/{clone.name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkoINU6OnufV"
      },
      "source": [
        "## 2. Manual labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_O1GHHRnufV"
      },
      "source": [
        "Before we can train a segmentation model, we need some data. Thus, we'll start by labeling a subset of our data manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovrTcc7CnufV"
      },
      "source": [
        "Open [Segments.ai](https://segments.ai/home) in your browser and navigate to your newly created dataset. Press \"Start Labeling\" to open the labeling interface and label your first point cloud. If you don't know how to use the labeling interface, have a look at [the docs](https://docs.segments.ai/guides/use-the-labeling-interfaces/3d-point-cloud-segmentation-interface). Keep in mind you can use hotkeys (and [customize them](https://docs.segments.ai/guides/customize-hotkeys)) to label faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1W20XrrnufW"
      },
      "source": [
        "## 3. Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sizHJFAbnufW"
      },
      "source": [
        "Now that we have some labeled data, we can train a model. To do this, we'll first create a release of our dataset, and turn it into a [`SegmentsDataset`](https://segments-python-sdk.readthedocs.io/en/latest/dataset.html) with the labeled samples. Then we'll train a segmentation on these labeled samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzPvD5YtnufW"
      },
      "outputs": [],
      "source": [
        "release_name = \"v0.1\"\n",
        "client.add_release(dataset_identifier, release_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwPpLL-CnufW"
      },
      "source": [
        "*Creating a release can take a short while, so you might run into problems if you immediately execute the next cell. You can check the status on the releases tab of your dataset in the web interface.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fwxkcF6vWCH"
      },
      "outputs": [],
      "source": [
        "release = client.get_release(dataset_identifier, release_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nb4Z1hxnufW"
      },
      "outputs": [],
      "source": [
        "from segments import SegmentsDataset\n",
        "\n",
        "dataset = SegmentsDataset(\n",
        "    release, labelset=\"ground-truth\", filter_by=[\"LABELED\", \"REVIEWED\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rat3SaJNnufX"
      },
      "source": [
        "For demonstration purposes, we'll cheat and simply use a pretrained [SqueezeSegV3](https://github.com/chenfengxu714/SqueezeSegV3) model here. Run the next cell to install the requirements for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MKXihHxnufX"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/chenfengxu714/SqueezeSegV3.git -q\n",
        "! pip install -r SqueezeSegV3/requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-bvKBf4nufX"
      },
      "source": [
        "## 4. Generate and upload label predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypvYELEHnufX"
      },
      "source": [
        "Now that we have a trained model, we can run it on the unlabeled point clouds to generate label predictions. Then we can upload these predictions to Segments.ai to correct any mistakes our model still made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcM58nrPnufX"
      },
      "source": [
        "We'll start by creating a new `SegmentsDataset` containing the unlabeled frames, and downloading the point clouds. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6fHSv79nufX"
      },
      "outputs": [],
      "source": [
        "dataset = SegmentsDataset(release, labelset=\"ground-truth\", filter_by=\"UNLABELED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrXNeDVynufY"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "dataset_path = \"./unlabeled_data\"\n",
        "\n",
        "download_path = os.path.join(dataset_path, \"sequences\", \"00\", \"velodyne\")\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "for sample in dataset:\n",
        "    # Download each point cloud\n",
        "    sample_url = sample[\"attributes\"][\"pcd\"][\"url\"]\n",
        "    urllib.request.urlretrieve(\n",
        "        sample_url, os.path.join(download_path, f'{sample[\"name\"]}.bin')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7MkfgdHnufY"
      },
      "source": [
        "Now, we can run the model on the unlabeled frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CuOQ8EocnufY"
      },
      "outputs": [],
      "source": [
        "from utils import run_model\n",
        "\n",
        "output_path = \"./output\"\n",
        "run_model(dataset_path, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFawoo2nufY"
      },
      "source": [
        "Finally, we can upload the predictions to Segments.ai using [`client.add_label()`](https://segments-python-sdk.readthedocs.io/en/latest/client.html#create-a-label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1RvbBGGnufY"
      },
      "outputs": [],
      "source": [
        "from utils import get_prediction\n",
        "\n",
        "predictions_path = os.path.join(output_path, \"sequences\", \"00\", \"predictions\")\n",
        "\n",
        "for sample in dataset:\n",
        "    name = sample[\"name\"]\n",
        "    label_path = os.path.join(predictions_path, name + \".label\")\n",
        "    annotations, point_annotations = get_prediction(label_path)\n",
        "\n",
        "    # Upload the predictions to Segments.ai\n",
        "    attributes = {\n",
        "        \"format_version\": \"0.2\",\n",
        "        \"annotations\": annotations,\n",
        "        \"point_annotations\": point_annotations,\n",
        "    }\n",
        "    client.add_label(\n",
        "        sample[\"uuid\"], \"ground-truth\", attributes, label_status=\"PRELABELED\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQJjhAInufY"
      },
      "source": [
        "## 5. Correct and repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sST6dAShnufY"
      },
      "source": [
        "Now go back to [Segments.ai](https://segments.ai/home) and click the \"Start labeling\" button to start labeling again. This time, your job is quite a bit easier: instead of having to label each image from scratch, you can simply correct the mistakes your model made.\n",
        "\n",
        "After labeling some more images, you can go back to step 4 and retrain your model. This way, it will become increasingly easy to label point clouds. After some iterations, you might reach a point where you're mostly just verifying the model's predictions, only having to correct the occasional mistakes on hard edge cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNeoebsUnufY"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhyT6v9OnufY"
      },
      "source": [
        "### Create your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5iflFhfnufZ"
      },
      "source": [
        "Start by creating an empty dataset using [`client.add_dataset()`](https://segments-python-sdk.readthedocs.io/en/latest/client.html#create-a-dataset). You have to specify a name, the task type (point cloud segmentation), and the categories you want to annotate in the point clouds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmX3gGtSnufZ"
      },
      "outputs": [],
      "source": [
        "name = \"my-point-clouds\"\n",
        "task_type = \"pointcloud-segmentation\"\n",
        "task_attributes = {\n",
        "    \"format_version\": \"0.1\",\n",
        "    \"categories\": [\n",
        "        {\"name\": \"ground\", \"id\": 1},\n",
        "        {\"name\": \"obstacle\", \"id\": 2},\n",
        "    ],\n",
        "}\n",
        "\n",
        "dataset = client.add_dataset(name, task_type=task_type, task_attributes=task_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsv5zLDcnufZ"
      },
      "source": [
        "Next, import your data. You can either upload point cloud files to Segments.ai's asset storage service, or pass URLs to the files on other cloud buckets. You can find more information in ([the docs](https://docs.segments.ai/guides/import-data)).\n",
        "\n",
        "Segments.ai currently supports [PCD](https://docs.segments.ai/reference/sample-types/supported-file-formats#pcd-point-cloud-data) and [binary XYZI(R)](https://docs.segments.ai/reference/sample-types/supported-file-formats#binary-xyzi-r-kitti-nuscenes) files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWCgh6VanufZ"
      },
      "outputs": [],
      "source": [
        "# Upload a local file\n",
        "filename = \"ca9a282c9e77460f8360f564131a8af5_nuscenes.bin\"\n",
        "\n",
        "with open(f\"path/to/{filename}\", \"rb\") as f:\n",
        "    asset = client.upload_asset(f, filename=filename)\n",
        "\n",
        "point_cloud_url = asset[\"url\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYn5fYe6nufZ"
      },
      "outputs": [],
      "source": [
        "dataset = \"tobias-admin/my-point-clouds\"\n",
        "\n",
        "name = \"ca9a282c9e77460f8360f564131a8af5_nuscenes\"\n",
        "\n",
        "attributes = {\n",
        "    \"pcd\": {\"url\": point_cloud_url, \"type\": \"nuscenes\"},\n",
        "}\n",
        "\n",
        "sample = client.add_sample(dataset, name, attributes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fast-point-cloud-segmentation.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d057461536556bff70246ab08987adf1b0948834940437d036dd23d29ab5014b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('local-sdk-env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
