{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Run the following cell to install the dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "! pip install segments-ai\n",
    "! git clone https://github.com/chenfengxu714/SqueezeSegV3.git\n",
    "! cd SqueezeSegV3\n",
    "! pip install -r requirements.txt\n",
    "! cd .."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quickly label point clouds by using model predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Upload your point clouds and label a small subset\n",
    "\n",
    "If you have a folder of point cloud on your pc, you can simply upload them to Segments.ai through the web interface: first create a new dataset, then upload the samples.\n",
    "\n",
    "But let's assume your data is in the cloud, and all you have is a list of image URLs. In this case, you can upload them to Segments.ai using our API or Python SDK. You need an API key for this, which can be created on your [account page](https://segments.ai/account).\n",
    "\n",
    "In this tutorial, our goal is to label a dataset of 10 SemanticKITTI scenes. First, we will initialize the client:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from segments import SegmentsClient\n",
    "\n",
    "api_key = '2ee27d7436c6c56f9b04e1655b51234ac9d1e88e'\n",
    "api_url = 'http://localhost:5000/'\n",
    "client = SegmentsClient(api_key, api_url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will create a new dataset for point cloud segmentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_name = \"pcsemseg\"\n",
    "task_type = 'pointcloud-segmentation'\n",
    "\n",
    "dataset = client.add_dataset(dataset_name, task_type=task_type)\n",
    "dataset_identifier = dataset['owner']['username'] + \"/\" + dataset_name"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are ready to upload the point cloud scans to the newly created dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# Upload the samples to Segments.ai\n",
    "for filename in os.listdir('data/sequences/00/velodyne'):\n",
    "  with open(os.path.join('data/sequences/00/velodyne', filename), 'rb') as f:\n",
    "    pcd_asset = client.upload_asset(f, filename + '.bin')\n",
    "\n",
    "  attributes = {\n",
    "    \"pcd\": {\n",
    "        \"url\": pcd_asset['url']\n",
    "    },\n",
    "  }\n",
    "\n",
    "  try:\n",
    "    client.add_sample(dataset_identifier, filename, attributes)\n",
    "  except:\n",
    "    continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the images are uploaded, go to the web interface, open the new dataset, and click the \"Start labeling\" button on the samples tab. Rather than immediately labeling the entire dataset, let's start out by labeling 2 images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Train a segmentation model on the labeled images\n",
    "\n",
    "After you've labeled a few point clouds, go to the releases tab of your dataset and create a new release, for example with the name \"v0.1\". A release is a snapshot of your dataset at a particular point in time.\n",
    "\n",
    "Through the Python SDK, we can now initialize a SegmentsDataset from this release and visualize the labeled images. The SegmentsDataset is compatible with popular frameworks like PyTorch, Tensorflow and Keras."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from segments import SegmentsDataset\n",
    "\n",
    "# Initialize a dataset from the release file\n",
    "release = client.get_release(dataset_identifier, 'v0.1')\n",
    "print(release)\n",
    "dataset = SegmentsDataset(release, labelset='ground-truth', filter_by='labeled')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, you can train a segmentation model on the labeled samples.\n",
    "However, for demonstration purposes, we will simply use a pretrained [SqueezeSegV3](https://github.com/chenfengxu714/SqueezeSegV3) model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train your model here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Generate and upload label predictions for the unlabeled images\n",
    "\n",
    "Now that we have a trained model, we can run it on the unlabeled point clouds to generate label predictions, and upload these predictions to Segments.ai:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_path = './unlabeled_data'\n",
    "output_path = \"./output\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from segments import SegmentsDataset\n",
    "\n",
    "\n",
    "# Initialize a new dataset, this time containing only unlabeled images\n",
    "dataset = SegmentsDataset(release, labelset='ground-truth', filter_by='unlabeled')\n",
    "\n",
    "download_path = os.path.join(dataset_path, 'sequences', '00', 'velodyne')\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "for sample in dataset:\n",
    "  # Save \n",
    "  sample_url = sample['attributes']['pcd']['url']\n",
    "  urllib.request.urlretrieve(sample_url, os.path.join(download_path, sample['name']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from utils import run_model\n",
    "\n",
    "run_model(dataset_path, output_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from utils import get_prediction\n",
    "from segments import SegmentsDataset\n",
    "\n",
    "predictions_path = os.path.join(output_path, 'sequences', '00', 'predictions')\n",
    "\n",
    "for sample in dataset:\n",
    "    name = sample[\"name\"][:-4]\n",
    "    label_path = os.path.join(predictions_path, name + '.label')\n",
    "    annotations, point_annotations = get_prediction(label_path)\n",
    "\n",
    "    # Upload the predictions to Segments.ai\n",
    "    attributes = {\n",
    "        'format_version': '0.1',\n",
    "        \"annotations\": annotations,\n",
    "        \"point_annotations\": point_annotations\n",
    "    }\n",
    "    client.add_label(sample['uuid'], 'ground-truth', attributes, label_status='PRELABELED')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Verify and correct the predicted labels\n",
    "\n",
    "Now go back to Segments.ai and click the \"Start labeling\" button again to continue labeling. This time, your job is quite a bit easier: instead of having to label each image from scratch, you can simply correct the few mistakes your model made!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "As you keep iterating between model training and labeling in this manner, your model will quickly get better and better. You'll reach a point where you're mostly just verifying the model's predictions, only having to correct the occasional mistakes on hard edge cases.\n",
    "\n",
    "Was this useful for you? Let us know! Make sure to check out the Segments.ai [documentation](https://docs.segments.ai/python-sdk) and don't hesitate to [contact us](https://segments.ai/contact) if you have any questions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('SqueezeSegV3': venv)"
  },
  "interpreter": {
   "hash": "ae02aa9beedf1a594aed30427a1265bed750b037a0c16e74745d515a4833761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}