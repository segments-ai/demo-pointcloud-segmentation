{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoTsDAKNtpFG"
      },
      "source": [
        "# Quickly label point clouds by using pre-annotations\n",
        "In this tutorial, we will upload predicted labels (pre-annotations) to speed up **semantic point cloud labeling**. Our goal is to label 3 SemanticKITTI frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o15nd65176N"
      },
      "source": [
        "Run the following cell to install the dependencies in Colab.\n",
        "\n",
        "Additionally, be sure to use a GPU-powered runtime by selecting it under `Runtime > Change runtime type` in the top bar of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9BF5LBn176Q"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/segments-ai/demo-pointcloud-segmentation.git\n",
        "! pip install segments-ai\n",
        "%cd demo-pointcloud-segmentation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McR1vSpx176R"
      },
      "source": [
        "## 1. Upload your point cloud frames\n",
        "\n",
        "In this notebook, we will create and populate a dataset on Segments.ai programmatically. For this, you need an API key, which can be created on your [account page](https://segments.ai/account).\n",
        "\n",
        "First, we will initialize the client:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-ddkKDG176R"
      },
      "outputs": [],
      "source": [
        "from segments import SegmentsClient\n",
        "\n",
        "api_key = 'API_KEY'  # paste your API key here\n",
        "client = SegmentsClient(api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4zXEiEo176S"
      },
      "source": [
        "Next, we will create a new dataset for point cloud segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U1UAOcF176S"
      },
      "outputs": [],
      "source": [
        "from utils import get_kitti_attributes\n",
        "\n",
        "dataset_name = \"pointcloud-pre-annotations\"\n",
        "task_type = 'pointcloud-segmentation'\n",
        "\n",
        "task_attributes = get_kitti_attributes()\n",
        "\n",
        "dataset = client.add_dataset(dataset_name, task_type=task_type, task_attributes=task_attributes)\n",
        "dataset_identifier = dataset['owner']['username'] + \"/\" + dataset_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-sGx4J1176S"
      },
      "source": [
        "Now, we are ready to upload the point cloud frames to the newly created dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3t-oLZg176T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Upload the samples to Segments.ai\n",
        "for filename in os.listdir('data/sequences/00/velodyne'):\n",
        "  with open(os.path.join('data/sequences/00/velodyne', filename), 'rb') as f:\n",
        "    pcd_asset = client.upload_asset(f, filename + '.bin')\n",
        "\n",
        "  attributes = {\n",
        "    \"pcd\": {\n",
        "        \"url\": pcd_asset['url']\n",
        "    },\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    client.add_sample(dataset_identifier, filename, attributes)\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nvZAywn176T"
      },
      "source": [
        "Once the images are uploaded, go to the [web interface](https://segments.ai/home) and open the new dataset. In the samples tab you can see the uploaded frames. Try to label a frame now and see how long it takes you. Press the \"Save\" button when you're done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q6Xregq176U"
      },
      "source": [
        "## 2. Train a segmentation model (or set-up another algorithm)\n",
        "\n",
        "Now that you've labeled a frame, go to the releases tab of your dataset and create a new release, for example with the name \"v0.1\". A release is a snapshot of your dataset at a particular point in time.\n",
        "\n",
        "Through the Python SDK, we can now initialize a SegmentsDataset from this release. The SegmentsDataset is compatible with popular frameworks like PyTorch, Tensorflow and Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sl9dHQF176U"
      },
      "outputs": [],
      "source": [
        "from segments import SegmentsDataset\n",
        "\n",
        "# Initialize a dataset from the release file\n",
        "release = client.get_release(dataset_identifier, 'v0.1')\n",
        "print(release)\n",
        "dataset = SegmentsDataset(release, labelset='ground-truth', filter_by='labeled')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwvSuToh176U"
      },
      "source": [
        "Next, you could you  train a segmentation model on the manually labeled frames. You could also use another algorithm to create label predictions. \n",
        "\n",
        "For demonstration purposes, we will use a pretrained [SqueezeSegV3](https://github.com/chenfengxu714/SqueezeSegV3) model here.\n",
        "\n",
        "Run the next cell to install the requirements for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KtQetVu176V"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/chenfengxu714/SqueezeSegV3.git\n",
        "! pip install -r SqueezeSegV3/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdanixG176V"
      },
      "source": [
        "## 3. Generate and upload label predictions for the unlabeled images\n",
        "\n",
        "Now that we have a trained model, we can run it on the unlabeled point clouds to generate label predictions. Then we can upload these predictions to Segments.ai to speed up labeling, or to check the performance of the model visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JakYLuww176V"
      },
      "outputs": [],
      "source": [
        "dataset_path = './unlabeled_data'\n",
        "output_path = \"./output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFU-Efg3yHU7"
      },
      "source": [
        "Create a new `SegmentsDataset`, this time containing only unlabeled frames, and download the point cloud frames. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcUOxptn176V"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "from segments import SegmentsDataset\n",
        "\n",
        "\n",
        "dataset = SegmentsDataset(release, labelset='ground-truth', filter_by='unlabeled')\n",
        "\n",
        "download_path = os.path.join(dataset_path, 'sequences', '00', 'velodyne')\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "for sample in dataset:\n",
        "  # Save \n",
        "  sample_url = sample['attributes']['pcd']['url']\n",
        "  urllib.request.urlretrieve(sample_url, os.path.join(download_path, sample['name']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-AjS9g2yWW0"
      },
      "source": [
        "Use the model (or your own algorithm) to generate labels for the frames in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqS_NheM176W"
      },
      "outputs": [],
      "source": [
        "from utils import run_model\n",
        "\n",
        "run_model(dataset_path, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkLZBKG5yhtQ"
      },
      "source": [
        "Upload the predictions to Segments.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF5yYca-176W"
      },
      "outputs": [],
      "source": [
        "from utils import get_prediction\n",
        "from segments import SegmentsDataset\n",
        "\n",
        "predictions_path = os.path.join(output_path, 'sequences', '00', 'predictions')\n",
        "\n",
        "for sample in dataset:\n",
        "    name = sample[\"name\"][:-4]\n",
        "    label_path = os.path.join(predictions_path, name + '.label')\n",
        "    annotations, point_annotations = get_prediction(label_path)\n",
        "\n",
        "    # Upload the predictions to Segments.ai\n",
        "    attributes = {\n",
        "        'format_version': '0.1',\n",
        "        \"annotations\": annotations,\n",
        "        \"point_annotations\": point_annotations\n",
        "    }\n",
        "    client.add_label(sample['uuid'], 'ground-truth', attributes, label_status='PRELABELED')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBzveKE2176W"
      },
      "source": [
        "## 4. Verify and correct the predicted labels\n",
        "\n",
        "Now go back to [Segments.ai](https://segments.ai/home) and click the \"Start labeling\" button to start labeling. This time, your job is quite a bit easier: instead of having to label each image from scratch, you can simply correct the few mistakes your model made!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJEsxOst176W"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Using this workflow for using pre-annotations, you can iterate between labeling and model training. This way, your model will quickly get better and better. You'll reach a point where you're mostly just verifying the model's predictions, only having to correct the occasional mistakes on hard edge cases.\n",
        "\n",
        "Was this useful for you? Let us know! Make sure to check out the Segments.ai [documentation](https://docs.segments.ai/python-sdk) and don't hesitate to [contact us](https://segments.ai/contact) if you have any questions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pc-pre-annotations.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f7a90fcf80db4248971d4cbec38447c89c3b8d522ff7ec661c6e387fd2017be7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
